---
title: "Stats For Chapter 2"
output: html_document
date: "2025-02-27"
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, error = FALSE)

# packages
library(data.table)
library(tidyverse)
library(lme4)
library(emmeans)
library(lmerTest)
library(nlme)
library(car)
library(DHARMa)
library(glmmTMB)
library(MuMIn)
library(GGally)

# load in the data
path <- "C:/Users/PC/OneDrive - University of the Sunshine Coast/AnomalyDetection/Output/Combined/all_combined_metrics.csv"
  
  
original_df <- read.csv(path)

# recode some of the variables
original_df <- original_df %>%
  mutate(model_type = recode_factor(model_type,
                             "multi_Activity_NOthreshold" = "Control",
                             "oneclass" = "One-class",
                             "binary" = "Binary",
                             "multi_Activity_threshold" = "Threshold",
                             "multi_Other" = "Other")) %>%
  select(-c(Count, file, dataset))


# define my theme for all my plots
my_theme <- function() {
  theme(
# both axes
  axis.title = element_text(size = 14), 
  axis.text = element_text(size = 10),
  
  # x axis
  axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0),
  
  # legend
  legend.title = element_text(size = 12, face = "bold"), 
  legend.text = element_text(size = 10),
  legend.position = "right",
  legend.box.background = element_blank(),
  legend.key.size = unit(0.5, "cm"),
  
  # background of the plot
  panel.background = element_blank(),
  panel.border = element_rect(color = "black", fill = NA, linewidth = 2),
  panel.grid.major.y = element_line(color = "lightgrey", linetype = "dashed"),
  panel.grid.major.x = element_blank(),
  panel.grid.minor = element_blank(),
  plot.background = element_blank(),
  
  # facet wrapping
  strip.background = element_blank(),
  strip.text = element_text(size = 12)
  )
}

my_colours = c("tomato", "aquamarine3", "orchid3", "slateblue2", "goldenrod2", "deepskyblue3", "firebrick3", "tan1", "lightcoral" , "darkcyan", "coral")
  
```

## Introduction
In this study, I compared the performance of the control (standard multiclass classification) approach to 4 possible OSR models on increasingly open test data with 3 conditions: all, some, and target. I have trained and tested a model for each of these model_type * training_set scenarios. Here, I evaluate the following hypotheses using the collated performance scores:

1. Control models would exhibit significantly lower performance when tested on open compared to closed data.
2. All multiclass models (control, other, threshold) would exhibit significantly lower performance when tested on open compared to closed data, while divide-and-conquer strategies (one-class and binary) would not differ.
3. Combined predictions from multiple binary one-vs-all models would achieve the highest performance of all models tested on open data in conditions some and target.

## Evaluation Metrics
For these evaluations, I use three key metrics:
- Accuracy
- F1 score (harmonic mean of precision and recall)  
- Youden's J statistic (referred to as Open Set F score)

To account for differing numbers of classes between models, I compare observed predictions to chance predictions from a naive random guesser. For n classes, the chance levels are:
- Random accuracy = 1/n
- Random F1 = 1/n  
- Random Youden's J = 2/n - 1

I use weighted averages across all behaviors (target and non-target) to provide balanced comparisons between models.

```{r calculating random baselines, include = FALSE}
# calculate the number of classes
metrics_df <- original_df %>% 
  group_by(model_type, training_set, fold) %>% 
  summarize(
    n = length(unique(behaviour))-1, # account for weighted_avg not counting
    random_accuracy = 1/n,
    random_specificity = (n-1)/n,
    random_F1 = 1/n,
    random_Yoden = 2/n -1,
    random_AUC = 0.5,
    .groups = "drop"
  ) 

most_common_metrics <- metrics_df %>%
  group_by(model_type, training_set) %>%
  count(n, random_accuracy, random_specificity, random_F1, random_Yoden, random_AUC) %>%
  arrange(model_type, training_set, desc(nn)) %>%
  group_by(model_type, training_set) %>%
  slice(1) %>%
  ungroup() %>%
  select(-nn) # Remove the count column

# Now apply these most common values to all folds within each combination
standardized_metrics <- metrics_df %>%
  select(model_type, training_set, fold) %>%
  left_join(
    most_common_metrics,
    by = c("model_type", "training_set")
  )

# Join this with the original dataframe
df_full <- original_df %>%
  left_join(
    standardized_metrics,
    by = c("model_type", "training_set", "fold")
  ) 

df <- df_full%>%
  filter(behaviour == 'weighted_avg')

```

## Hypothesis 1: Effect of Open Set on Control Performance

I hypothesised that the control model would decline in performance when tested on open rather than closed sets. To test this, I isolated my results from my control models.

I plotted this relationship where the coral line is the performance of chance for each metric.

```{r hypo1.plot1, echo = FALSE, warning=FALSE}
# Create long format data focusing only on Accuracy and F1
plot_df <- df %>%
  select(model_type, training_set, behaviour, fold, 
         Accuracy, F1, random_accuracy, random_F1) %>%
  pivot_longer(cols = c(Accuracy, F1),
               names_to = "Metric",
               values_to = "Value") %>%
  filter(model_type == 'Control') %>%
  mutate(training_set = factor(training_set, levels = c("all", levels(factor(training_set))[levels(factor(training_set)) != "all"])),
         random_value = if_else(Metric == "Accuracy", random_accuracy, random_F1))

# Create plot with mean and error bars
hypo1 <- plot_df %>%
  group_by(training_set, Metric) %>%
  summarise(
    mean = mean(Value, na.rm = TRUE),
    se = sd(Value, na.rm = TRUE)/sqrt(n()),
    random_value = first(random_value),
    .groups = "drop"
  ) %>%
  ggplot(aes(x = training_set, y = mean)) +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = 0.2) +
  geom_point(size = 3) +
  geom_segment(aes(
    x = as.numeric(training_set) - 0.3,
    xend = as.numeric(training_set) + 0.3,
    y = random_value,
    yend = random_value
  ), color = "coral", linewidth = 1) +
  facet_grid(~ Metric) +
  my_theme() +
  labs(x = "Training Set", y = "Value")

print(hypo1)

```

Now I tested the hypothesis using a subset of the data where model is control. First I looked at the assumptions for a model.

```{r hypo1}
control_df <- df %>% filter(model_type == 'Control')

m1 <- glmmTMB(F1 ~ training_set + (1 | fold), data = control_df, family = gaussian)

# check the assumptions
sim_res<- simulateResiduals(m1)
sim_res_hypo1 <- plot(sim_res)
print(sim_res_hypo1)

```

And then I tested it anc compared each condition to the control.

```{r}
r.squaredGLMM(m1)
summary(m1)

emmeans(m1, specs = trt.vs.ctrl ~ training_set)$contrasts %>%
  summary(infer = TRUE)
```

Training set some and target both perform significantly worse than on closed data. I therefore considered that my first hypothesis was confirmed.

I looked at the incidence of false positives.

```{r}

m2 <- glmmTMB(FPR ~ training_set + (1 | fold), data = control_df, family = gaussian)

# check the assumptions
sim_res<- simulateResiduals(m2)
sim_res_hypo1 <- plot(sim_res)
print(sim_res_hypo1)

r.squaredGLMM(m2)
summary(m2)

emmeans(m2, specs = trt.vs.ctrl ~ training_set)$contrasts %>%
  summary(infer = TRUE) 

```

## Hypothesis 2: Performance Patterns Across Model Types

I hypothesised that the three multiclass models (control, threshold, and other) would all significantly degrade in performance between the all and reduced training conditions whereas the divide and conquer methods would not. 

Firstly I plotted this data.

```{r, echo=FALSE}
# Reshape data more efficiently, focusing only on Accuracy and F1
plot_data <- df %>%
  select(model_type, training_set, fold, 
         Accuracy, F1, random_accuracy, random_F1) %>%
  pivot_longer(cols = c(Accuracy, F1),
               names_to = "Metric",
               values_to = "Value") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "F1")),
         training_set = factor(training_set))

# Create summary statistics
summary_data <- plot_data %>%
  group_by(model_type, training_set, Metric) %>%
  summarise(
    mean = mean(Value, na.rm = TRUE),
    se = sd(Value, na.rm = TRUE)/sqrt(n()),
    .groups = "drop"
  )

# Create reference line data more concisely
random_lines <- df %>%
  select(model_type, training_set, random_accuracy, random_F1) %>%
  distinct() %>%
  pivot_longer(cols = c(random_accuracy, random_F1), 
               names_to = "Random_Metric", 
               values_to = "Random_Value") %>%
  mutate(
    Metric = factor(case_when(
      Random_Metric == "random_accuracy" ~ "Accuracy",
      Random_Metric == "random_F1" ~ "F1",
      TRUE ~ NA_character_
    ), levels = c("Accuracy", "F1")),
    training_set = factor(training_set),
    training_set_numeric = as.numeric(training_set)
  )

# Create plot with mean and error bars
hypo2 <- ggplot(summary_data, aes(x = training_set, y = mean)) +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = 0.2) +
  geom_point(size = 3) +
  geom_segment(data = random_lines,
               aes(x = training_set_numeric - 0.3,
                   xend = training_set_numeric + 0.3,
                   y = Random_Value, 
                   yend = Random_Value),
               color = "coral",
               linewidth = 1) +
  facet_grid(Metric ~ model_type) +
  my_theme() +
  labs(x = "Training Set", y = "Value")

print(hypo2)

```

An alternative way to look at this same data is:

```{r, echo = FALSE}
# Using the same plot_data from previous code
# Create summary statistics with model_type as x-axis and training_set as color
summary_data_by_model <- plot_data %>%
  group_by(model_type, training_set, Metric) %>%
  summarise(
    mean = mean(Value, na.rm = TRUE),
    se = sd(Value, na.rm = TRUE)/sqrt(n()),
    .groups = "drop"
  ) %>%
  mutate(model_type = factor(model_type))

# Update random lines for this plot orientation
random_lines_by_model <- random_lines %>%
  mutate(model_type_numeric = as.numeric(factor(model_type)))

# Create plot with model_type on x-axis and training_set as color
hypo2_2 <- ggplot(summary_data_by_model, 
                 aes(x = model_type, y = mean)) +
  geom_point(position = position_dodge(width = 0.7), size = 3) +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), 
                position = position_dodge(width = 0.7), 
                width = 0.2) +
  geom_segment(data = random_lines_by_model,
               aes(x = model_type_numeric - 0.3,
                   xend = model_type_numeric + 0.3,
                   y = Random_Value, 
                   yend = Random_Value),
               color = "coral",
               linewidth = 1) +
  facet_grid(Metric~ training_set) +
  my_theme() +
  labs(x = "Model Type", y = "Value", color = "Training Set")

print(hypo2_2)

```

Then I selected a statistical model and tested assumptions.

```{r}
m1 <- glmmTMB(F1 ~ model_type * training_set + (1 | fold), data = df)

sim_res<- simulateResiduals(m1)
sim_res_hypo2 <- plot(sim_res)
print(sim_res_hypo2)
```

It met assumptions, so I can analyse it.

```{r}
r.squaredGLMM(m1)
summary(m1)
```
Fold provided very little variance. In terms of fixed effects, 'One-class' performed significantly worse than the control, but no other models did. Target and some performed significantly worse than all. There were several interactions so I looked at these using emmeans.

```{r}
emmeans(m1, specs = trt.vs.ctrl ~ model_type | training_set)$contrasts %>%
  summary(infer = TRUE)
```

From these comparisons I can see that in condition all, One-class is significantly worse than control. In Some, Binary, Threshold, and Other are all better than control. There was no significant difference between models in training set target.


And the other direction:
```{r}
emmeans(m1, specs = trt.vs.ctrl ~ training_set | model_type)$contrasts %>%
  summary(infer = TRUE)
```

## Closer Look At Binary Models

I looked at Binary in detail.

```{r, echo = FALSE}
Other_df <- df_full %>% filter(
                          model_type == "Binary",
                          behaviour %in% c("eat", "lay", "walk", "Other", "weighted_avg"))

long_Other <- melt(Other_df, 
                id.vars = c("model_type", "training_set", "behaviour", "fold", "random_accuracy", "random_specificity", "random_F1"), 
                measure.vars = c("Accuracy", "F1", "YodenJ"), 
                variable.name = "Metric",
                value.name = "Value")

# Ensure Metric is a factor
long_Other$Metric <- factor(long_Other$Metric, levels = c("Accuracy", "F1", "YodenJ"))

# Create reference line data
random_lines <- Other_df %>%
  select(training_set, model_type, behaviour, random_accuracy, random_F1, random_Yoden) %>%
  distinct() %>%  # Ensure one row per training_set, model_type
  pivot_longer(cols = c(random_accuracy, random_F1, random_Yoden), 
               names_to = "Random_Metric", 
               values_to = "Random_Value") %>%
  mutate(Metric = factor(case_when(
    Random_Metric == "random_accuracy" ~ "Accuracy",
    Random_Metric == "random_F1" ~ "F1",
    Random_Metric == "random_Yoden" ~ "YodenJ",
    TRUE ~ NA_character_
  ), levels = c("Accuracy", "F1", "YodenJ"))) %>%
  filter(!is.na(Metric))

# Convert training_set to factor for consistency
long_Other$training_set <- as.factor(long_Other$training_set)
random_lines$training_set <- as.factor(random_lines$training_set)

# Convert training_set to numeric for reference lines
random_lines <- random_lines %>%
  mutate(training_set_numeric = as.numeric(training_set)) %>%
  mutate(model_type_numeric = as.numeric(model_type))

long_Other$behaviour <- factor(long_Other$behaviour, 
                               levels = c("walk", "eat", "lay", "Other", "weighted_avg"))
  
Other_hypo2 <- ggplot(long_Other, aes(x = training_set, y = Value, colour = behaviour)) +
  geom_boxplot(linewidth = 1, fatten = 1, outlier.size = 2) +
  scale_color_manual(values = my_colours) +
  geom_segment(data = random_lines,
               aes(x = training_set_numeric - 0.3,
                   xend = training_set_numeric + 0.3,
                   y = Random_Value, 
                   yend = Random_Value,
                   group = Metric),
               color = "coral",
               linewidth = 1) +
  facet_wrap(~ Metric, scales = "free_y") +
  my_theme() +
  labs(x = "Training Set", 
       y = "Value")

print(Other_hypo2)

```

Finally I want to generate a nicer plot for the False Positive Rate. This will require reading in all of the confusion matrices for the control models from each of the folds. I will do this once and then save the csv.

```{r false positives, fig.width=10,fig.height=3.5}

all_files <- list.files(
  "C:/Users/PC/OneDrive - University of the Sunshine Coast/AnomalyDetection/Output", 
  recursive = TRUE, 
  full.names = TRUE
)

# Filter to select the control models
control_files <- all_files[grep("ConfusionMatrices/Ferdinandy_Dog_.*_multi_Activity.*NOthreshold_fullclasses_confusion_matrix\\.csv$", all_files)]


all_confusion <- bind_rows(lapply(control_files, function(file_path) {
  # Extract fold number
  fold <- str_extract(file_path, "fold_\\d+")
  fold <- str_replace(fold, "fold_", "")  # Clean up "fold_"
  
  # Extract condition (directly after "Ferdinandy_Dog_")
  condition <- str_extract(file_path, "Ferdinandy_Dog_([^_]+)")
  condition <- str_replace(condition, "Ferdinandy_Dog_", "")  # Clean up "Ferdinandy_Dog_"
  
  # Read the file
  file <- fread(file_path)
  
  # Reshape and clean the data
  file_long <- melt(file, 
                    id.vars = c("V1"), 
                    measure.vars = colnames(file)[2:length(colnames(file))], 
                    variable.name = "Predicted_class",
                    value.name = "Count") %>%
    rename("True_class" = "V1") %>%
    mutate(Fold = fold,
           Condition = condition)
  
  return(file_long)
}))

means <- all_confusion %>% group_by(True_class, Predicted_class, Condition) %>% summarise(Count_mean = mean(Count))

means$True_class <- factor(means$True_class, levels = c("Other", "walk", "stand", "lay", "trot", "run", "sit", "eat", "drink"))

ggplot(means, aes(x = True_class, y = Count_mean, fill = Predicted_class)) +
  geom_bar(position="stack", stat="identity") + facet_wrap(~ Condition) +
  my_theme() +
  scale_fill_manual(values = my_colours) +
  labs(y = "Predicted Class", x = "True Class")

ggplot(means, aes(x = True_class, y = Count_mean, fill = Predicted_class)) +
  geom_bar(position="fill", stat="identity") + facet_wrap(~ Condition) +
  my_theme() +
  scale_fill_manual(values = my_colours)+
  labs(y = "Proportion Predicted Class", x = "True Class")

```



