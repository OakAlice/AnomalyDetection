---
title: "Stats For Chapter 2"
output: html_document
date: "2025-02-27"
---

## Introduction
Behaviour classification from animal accelerometer data is an inherently open set recognition (OSR) task, but this fact is rarely recognised. In this study, I compared the performance of the control (standard multiclass classification) approach to 4 possible OSR models on increasingly open test data with 3 conditions: all, some, and target. I have now trained and tested a model for each of these model type x condition scenarios. I have collated the performance scores from each and am now evaluating my hypotheses.

* 1: Control models would exhibit significantly lower performance when tested on open compared to closed data.
* 2: All multiclass models (control, other, threshold) would exhibit significantly lower performance when tested on open compared to closed data, while divide-and-conquer strategies (one-class and binary) would not differ.
* 3: Combined predictions from multiple binary one-vs-all models would achieve the highest performance of all models tested on open data in conditions some and target.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# packages
library(data.table)
library(tidyverse)
library(lme4)
library(emmeans)
library(lmerTest)
library(nlme)

# load in the data
path <- "C:/Users/PC/OneDrive - University of the Sunshine Coast/AnomalyDetection/Scripts/all_combined_metrics.csv"
df <- read.csv(path)

# define some variables
target_behaviours <- c("eat", "lay", "walk", "Other")

# recode some of the variables
df <- df %>%
  mutate(model_type = recode(model_type,
                             "multi_Activity_NOthreshold" = "Control",
                             "binary" = "Binary",
                             "oneclass" = "One-class",
                             "multi_Activity_threshold" = "Threshold",
                             "multi_Other" = "Other"))

# define my theme for all my plots
my_theme <- function() {
  theme(
# both axes
  axis.title = element_text(size = 14), 
  axis.text = element_text(size = 10),
  
  # x axis
  axis.text.x = element_text(angle = -20, vjust = 1, hjust = 0),
  
  # legend
  legend.title = element_text(size = 12, face = "bold"), 
  legend.text = element_text(size = 10),
  legend.position = "right",
  legend.box.background = element_blank(),
  legend.key.size = unit(1, "cm"),
  
  # background of the plot
  panel.background = element_blank(),
  panel.border = element_rect(color = "black", fill = NA, size = 2),
  panel.grid.major.y = element_line(color = "lightgrey", linetype = "dashed"),
  panel.grid.major.x = element_blank(),
  panel.grid.minor = element_blank(),
  plot.background = element_blank(),
  plot.margin = margin(1, 1, 1, 1, "cm"),
  
  # facet wrapping
  strip.background = element_blank(),
  strip.text = element_text(size = 12)
  )
}
  
```

## Evaluation metrics
I used the metric AUC for model tuning as it does not require the selection of a threshold and has a built-in mechanism for accounting for chance (0.5 is chance rate). For the final evaluations, I will use accuracy, which measures the proportion of correct predictions of total predictions, and specificity which is 1 - false positive rate. With these two metrics I will be able to see both the positive predictions and the ability of the model to avoid over-predicting the known classes. When using these metrics to compare models with differing amounts of classes, however, we have to account for the relative rate of chance (i.e., the probability of a correct prediction is higher in a task with fewer possible classes, so we need to correct for that).

The simplest way to account for chance is to compare the observed predictions to the chance predictions of a naive random guesser. Naive random guessers guess each class in equal proportion. Accuracy for each class will therefore be 1/n and specificity will be (k-1)/k. 

Therefore, I calculated these chance rates for each model type and used these to adjust the observed values. I used these adjusted metrics to test each of my hypotheses.

```{r adjusting for chance, warning=FALSE, include=FALSE}

chance <- df %>% 
  group_by(model_type, training_set, closed_open, fold) %>%
  count() %>%
  mutate(chance_accuracy = 1/n,
         chance_specificity = (n-1)/n) %>%
  ungroup() %>%
  select(-c("fold", "n"))

df <- merge(chance, df, by = c("model_type", "training_set", "closed_open"))

df <- df %>%
  mutate(Adj_Accuracy = Accuracy - chance_accuracy,
         Adj_Specificity = Specificity - chance_specificity) %>%
  select(-chance_accuracy, chance_specificity)

# rearrange it to be long for plotting
long_df <- melt(df, 
                id.vars = c("model_type", "training_set", "closed_open", "behaviour", "fold"), 
                measure.vars = c("Adj_Accuracy", "Adj_Specificity"), 
                variable.name = "Metric",
                value.name = "Value") 

```

## Hypothesis 1

I hypothesised that the control model would decline in performance when tested on open rather than closed sets. To test this, I isolated my results from my control models. I used the weighted average of each fold

I plotted this relationship.

```{r hypo1.plot1}
subset_df <- long_df %>% filter(behaviour == 'weighted_avg',
                           model_type == 'Control')
subset_df$training_set <- relevel(factor(subset_df$training_set), ref = "all")

ggplot(subset_df, aes(x = training_set, y = Value))+
  geom_boxplot()+
  facet_grid(Metric~closed_open) +
  my_theme()

```

Firstly I showed that when these models were tested on closed sets.

```{r hypo1.part1}
subset_df <- df %>% filter(closed_open == "closed",
                           behaviour == 'weighted_avg',
                           model_type == 'Control')

acc_aov_model <- aov(Adj_Accuracy ~training_set, data = subset_df)
summary(acc_aov_model)
TukeyHSD(acc_aov_model)

spec_aov_model <- aov(Adj_Specificity ~training_set, data = subset_df)
summary(spec_aov_model)
TukeyHSD(spec_aov_model)

```
I then tested whether this changed when tested on open sets.

```{r hypo1.part2}
subset_df <- df %>% filter(closed_open == "open",
                           behaviour == 'weighted_avg',
                           model_type == 'Control')
subset_df$training_set <- relevel(factor(subset_df$training_set), ref = "all")

acc_aov_model <- aov(Adj_Accuracy ~training_set, data = subset_df)
summary(acc_aov_model)
TukeyHSD(acc_aov_model)

spec_aov_model <- aov(Adj_Specificity ~training_set, data = subset_df)
summary(spec_aov_model)
TukeyHSD(spec_aov_model)
```

## Hypothesis 2

I hypothesised that the three multiclass models (control, threshold, and other) would all significantly degrade in performance between the all and reduced training conditions whereas the divide and conquer methods would not. Firstly I plotted it.

```{r hypo2.plot1}
subset_df2 <- long_df %>% filter(closed_open == "open",
                           behaviour == "weighted_avg")
subset_df2$training_set <- relevel(factor(subset_df2$training_set), ref = "all")
subset_df2$model_type <- factor(subset_df2$model_type, levels = c("Control", "One-class", "Binary", "Threshold", "Other"))
subset_df2$model_type <- relevel(factor(subset_df2$model_type), ref = "Control")

ggplot(data = subset_df2, aes(x = training_set, y = Value)) +
  geom_boxplot() +
  facet_grid(Metric~model_type) +
  my_theme()

```

First, I checked whether there was a significant interaction between the two variables.

```{r hypo2.part1}
subset_df3 <- subset_df2[subset_df2$Metric == "Adj_Accuracy", ]
lme_model3 <- lmer(Value ~ training_set * model_type + (1 | fold), data = subset_df3)
summary(lme_model3)

subset_df4 <- subset_df2[subset_df2$Metric == "Adj_Specificity", ]
lme_model4 <- lmer(Value ~ training_set * model_type + (1 | fold), data = subset_df4)
summary(lme_model4)
```
There were significant interactions.

```{r hypo2.part3}
pairwise <- emmeans(lme_model3, pairwise ~ model_type | training_set, adjust = "tukey")
pairwise$contrasts
```




