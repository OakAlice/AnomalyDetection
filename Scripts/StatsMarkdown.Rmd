---
title: "Stats For Chapter 2"
output: html_document
date: "2025-02-27"
---

## Introduction
In this study, I compared the performance of the control (standard multiclass classification) approach to 4 possible OSR models on increasingly open test data with 3 conditions: all, some, and target. I have trained and tested a model for each of these model type x condition scenarios. Here, I evaluate the following hypotheses using the collated performance scores:

1. Control models would exhibit significantly lower performance when tested on open compared to closed data.
2. All multiclass models (control, other, threshold) would exhibit significantly lower performance when tested on open compared to closed data, while divide-and-conquer strategies (one-class and binary) would not differ.
3. Combined predictions from multiple binary one-vs-all models would achieve the highest performance of all models tested on open data in conditions some and target.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, error = FALSE)

# packages
library(data.table)
library(tidyverse)
library(lme4)
library(emmeans)
library(lmerTest)
library(nlme)
library(car)
library(DHARMa)
library(glmmTMB)
library(MuMIn)

# load in the data
path <- "C:/Users/PC/OneDrive - University of the Sunshine Coast/AnomalyDetection/Scripts/all_combined_metrics.csv"
original_df <- read.csv(path)

# recode some of the variables
original_df <- original_df %>%
  mutate(model_type = recode_factor(model_type,
                             "multi_Activity_NOthreshold" = "Control",
                             "binary" = "Binary",
                             "oneclass" = "One-class",
                             "multi_Activity_threshold" = "Threshold",
                             "multi_Other" = "Other")) %>%
  select(-c(Support, Count, file, dataset))


# define my theme for all my plots
my_theme <- function() {
  theme(
# both axes
  axis.title = element_text(size = 14), 
  axis.text = element_text(size = 10),
  
  # x axis
  axis.text.x = element_text(angle = -20, vjust = 1, hjust = 0),
  
  # legend
  legend.title = element_text(size = 12, face = "bold"), 
  legend.text = element_text(size = 10),
  legend.position = "right",
  legend.box.background = element_blank(),
  legend.key.size = unit(1, "cm"),
  
  # background of the plot
  panel.background = element_blank(),
  panel.border = element_rect(color = "black", fill = NA, linewidth = 2),
  panel.grid.major.y = element_line(color = "lightgrey", linetype = "dashed"),
  panel.grid.major.x = element_blank(),
  panel.grid.minor = element_blank(),
  plot.background = element_blank(),
  plot.margin = margin(1, 1, 1, 1, "cm"),
  
  # facet wrapping
  strip.background = element_blank(),
  strip.text = element_text(size = 12)
  )
}
  
```

## Evaluation Metrics
For these evaluations, I use three key metrics:
- Accuracy
- F1 score (harmonic mean of precision and recall)  
- Youden's J statistic (referred to as Open Set F score)

To account for differing numbers of classes between models, I compare observed predictions to chance predictions from a naive random guesser. For n classes, the chance levels are:
- Random accuracy = 1/n
- Random F1 = 1/n  
- Random Youden's J = 2/n - 1

I use weighted averages across all behaviors (target and non-target) to provide balanced comparisons between models.

```{r calculating random baselines, include = FALSE}
# calculate the number of classes
metrics_df <- original_df %>% 
  group_by(model_type, training_set, closed_open, fold) %>% 
  summarize(
    n = n(),
    random_accuracy = 1/n,
    random_specificity = (n-1)/n,
    random_F1 = 1/n,
    random_Yoden = 2/n -1,
    .groups = "drop"
  ) 

most_common_metrics <- metrics_df %>%
  group_by(model_type, training_set, closed_open) %>%
  count(n, random_accuracy, random_specificity, random_F1, random_Yoden) %>%
  arrange(model_type, training_set, closed_open, desc(nn)) %>%
  group_by(model_type, training_set, closed_open) %>%
  slice(1) %>%
  ungroup() %>%
  select(-nn) # Remove the count column

# Now apply these most common values to all folds within each combination
standardized_metrics <- metrics_df %>%
  select(model_type, training_set, closed_open, fold) %>%
  left_join(
    most_common_metrics,
    by = c("model_type", "training_set", "closed_open")
  )

# Join this with the original dataframe
df_full <- original_df %>%
  left_join(
    standardized_metrics,
    by = c("model_type", "training_set", "closed_open", "fold")
  ) 

df <- df_full%>%
  filter(behaviour == 'weighted_avg')

```

## Hypothesis 1: Effect of Open Set on Control Performance

I hypothesised that the control model would decline in performance when tested on open rather than closed sets. To test this, I isolated my results from my control models.

I plotted this relationship where the coral line is the performance of chance for each metric.

```{r hypo1.plot1, echo = FALSE}
# rearrange it to be long for plotting
long_df <- melt(df, 
                id.vars = c("model_type", "training_set", "closed_open", "behaviour", "fold", "random_accuracy", "random_specificity", "random_F1"), 
                measure.vars = c("Accuracy", "Specificity", "F1"), 
                variable.name = "Metric",
                value.name = "Value") 

subset_df <- long_df %>% filter(model_type == 'Control')
subset_df$training_set <- relevel(factor(subset_df$training_set), ref = "all")

hypo1 <- ggplot(subset_df, aes(x = training_set, y = Value)) +
  geom_boxplot() +

  # Accuracy
  geom_segment(data = subset_df %>% 
                 filter(Metric == "Accuracy") %>%
                 distinct(training_set, closed_open, random_accuracy) %>%
                 mutate(Metric = factor("Accuracy")),  # Ensure Metric is present
               aes(x = as.numeric(training_set) - 0.3, 
                   xend = as.numeric(training_set) + 0.3,
                   y = random_accuracy, 
                   yend = random_accuracy,
                   group = Metric),  # Match facet variable
               color = "coral", 
               linewidth = 1) +
  
  # F1
  geom_segment(data = subset_df %>% 
                 filter(Metric == "F1") %>%
                 distinct(training_set, closed_open, random_F1) %>%
                 mutate(Metric = factor("F1")),  # Ensure Metric is present
               aes(x = as.numeric(training_set) - 0.3, 
                   xend = as.numeric(training_set) + 0.3,
                   y = random_F1, 
                   yend = random_F1,
                   group = Metric),  # Match facet variable
               color = "coral", 
               linewidth = 1) +

  # Specificity
  geom_segment(data = subset_df %>% 
                 filter(Metric == "Specificity") %>%
                 distinct(training_set, closed_open, random_specificity) %>%
                 mutate(Metric = factor("Specificity")),  # Ensure Metric is present
               aes(x = as.numeric(training_set) - 0.3, 
                   xend = as.numeric(training_set) + 0.3,
                   y = random_specificity, 
                   yend = random_specificity,
                   group = Metric),  # Match facet variable
               color = "coral", 
               linewidth = 1) +

  facet_grid(Metric ~ closed_open) +
  my_theme() +
  labs(x = "Training Set",
       y = "Value")

print(hypo1)

```

Now I tested the hypothesis using a subset of the data where model is control. First I identified the best explanatory model.

```{r hypo1}
control_df <- df %>% filter(model_type == 'Control')

m1 <- glmmTMB(F1 ~ closed_open * training_set + (1 | fold) + (1 | training_set), data = control_df, family = Gamma(link = "log"))
m2 <- glmmTMB(F1 ~ closed_open * training_set + (1 | fold), data = control_df)
m3 <- glmmTMB(F1 ~ closed_open * training_set, data = control_df)
m4 <- glmmTMB(F1 ~ closed_open + training_set, data = control_df)

# AIC
AIC(m1, m2, m3, m4)
# BIC
AIC(m1, m2, m3, m4, k=log(nrow(control_df)))
# Anova comparison
anova(m3, m4, test = "Chisq")

# check the assumptions
sim_res<- simulateResiduals(m3)
sim_res_hypo1 <- plot(sim_res)
print(sim_res_hypo1)
```

m3 was found to be the most explanatory variable but assumptions were not met... Pause here... Not sure what to do.

## Hypothesis 2: Performance Patterns Across Model Types

I hypothesised that the three multiclass models (control, threshold, and other) would all significantly degrade in performance between the all and reduced training conditions whereas the divide and conquer methods would not. 

Firstly I plotted this data.

```{r, echo=FALSE}
open_df <- df %>% filter(closed_open == "open")
# Reshape data
long_df <- melt(open_df, 
                id.vars = c("model_type", "training_set", "fold", "random_accuracy", "random_specificity", "random_F1"), 
                measure.vars = c("Accuracy", "F1", "YodenJ"), 
                variable.name = "Metric",
                value.name = "Value")

# Ensure Metric is a factor
long_df$Metric <- factor(long_df$Metric, levels = c("Accuracy", "F1", "YodenJ"))

# Create reference line data
random_lines <- open_df %>%
  select(training_set, model_type, random_accuracy, random_F1, random_Yoden) %>%
  distinct() %>%  # Ensure one row per training_set, model_type
  pivot_longer(cols = c(random_accuracy, random_F1, random_Yoden), 
               names_to = "Random_Metric", 
               values_to = "Random_Value") %>%
  mutate(Metric = factor(case_when(
    Random_Metric == "random_accuracy" ~ "Accuracy",
    Random_Metric == "random_F1" ~ "F1",
    Random_Metric == "random_Yoden" ~ "YodenJ",
    TRUE ~ NA_character_
  ), levels = c("Accuracy", "F1", "YodenJ"))) %>%
  filter(!is.na(Metric))

# Convert training_set to factor for consistency
long_df$training_set <- as.factor(long_df$training_set)
random_lines$training_set <- as.factor(random_lines$training_set)

# Convert training_set to numeric for reference lines
random_lines <- random_lines %>%
  mutate(training_set_numeric = as.numeric(training_set)) %>%
  mutate(model_type_numeric = as.numeric(model_type))

hypo2 <- ggplot(long_df, aes(x = training_set, y = Value)) +
  geom_boxplot() +

  # Add reference lines for random values
  geom_segment(data = random_lines,
               aes(x = training_set_numeric - 0.3,
                   xend = training_set_numeric + 0.3,
                   y = Random_Value, 
                   yend = Random_Value,
                   group = Metric),
               color = "coral",
               linewidth = 1) +

  facet_grid(Metric ~ model_type) +
  my_theme() +
  labs(x = "Training Set", 
       y = "Value")

print(hypo2)

```

An alternative way to look at this same data is:
```{r, echo = FALSE}
hypo2_2 <- ggplot(long_df, aes(x = model_type, y = Value)) +
  geom_boxplot() +

  # Add reference lines for random values
  geom_segment(data = random_lines,
               aes(x = model_type_numeric - 0.3,
                   xend = model_type_numeric + 0.3,
                   y = Random_Value, 
                   yend = Random_Value,
                   group = Metric),
               color = "coral",
               linewidth = 1) +

  facet_grid(Metric ~ training_set) +
  my_theme() +
  labs(x = "Model type", 
       y = "Value")

print(hypo2_2)

```

Then I selected a statistical model. I compared the more complex with reduced models to see which of them added necessary information.

```{r, include = FALSE}
m1 <- glmmTMB(F1 ~ model_type * training_set + (1 | fold), data = open_df)
m2 <- glmmTMB(F1 ~ model_type * training_set, data = open_df)
m3 <- glmmTMB(F1 ~ model_type + training_set, data = open_df)

# Anova comparison
anova(m1, m2, m3,test = "Chisq")
```

These tests show that the m3 (simplest model) is a better representation but the interaction term in m2 adds significant more information. As the folds are not independent, however, I have to select the model with the mixed effect.

```{r}
sim_res<- simulateResiduals(m1)
sim_res_hypo2 <- plot(sim_res)
print(sim_res_hypo2)
```

It met assumptions, so I can analyse it.

```{r}
r.squaredGLMM(m1)
summary(m1)
```

Fold provided very little variance. F1, 'One-class' performed significantly worse than the control, but no other models did. I found that target and some performed significantly worse than all. For the interactions:
- Binary and one-class significantly better than control in 'some'
- Threshold and one-class significantly better than control in 'target'

I then selected the same kind of model for Yoden's J and checked assumptions.
```{r, include = FALSE}
m1 <- glmmTMB(YodenJ ~ model_type * training_set + (1 | fold), data = open_df)

sim_res<- simulateResiduals(m1)
sim_res_hypo2_2 <- plot(sim_res)
print(sim_res_hypo2_2)
```

```{r}
r.squaredGLMM(m1)
summary(m1)
```

Again variance from folds were small. One-class performed significantly worse and all other metrics were the same as control. Both open sets performed worse than the closed set. For the interactions:
- Binary significantly better than control in 'some'
- Threshold significantly better than control in 'target'

### More detail about the significant models

For the models that suggested better weighted average than control performance, I looked at individual behaviours. First I looked at Threshold.

```{r, echo = FALSE}
Other_df <- df_full %>% filter(closed_open == "open",
                          model_type == "Threshold",
                          behaviour %in% c("eat", "lay", "walk", "Other", "weighted_avg"))

long_Other <- melt(Other_df, 
                id.vars = c("model_type", "training_set", "behaviour", "fold", "random_accuracy", "random_specificity", "random_F1"), 
                measure.vars = c("Accuracy", "F1", "YodenJ"), 
                variable.name = "Metric",
                value.name = "Value")

# Ensure Metric is a factor
long_Other$Metric <- factor(long_Other$Metric, levels = c("Accuracy", "F1", "YodenJ"))

# Create reference line data
random_lines <- Other_df %>%
  select(training_set, model_type, behaviour, random_accuracy, random_F1, random_Yoden) %>%
  distinct() %>%  # Ensure one row per training_set, model_type
  pivot_longer(cols = c(random_accuracy, random_F1, random_Yoden), 
               names_to = "Random_Metric", 
               values_to = "Random_Value") %>%
  mutate(Metric = factor(case_when(
    Random_Metric == "random_accuracy" ~ "Accuracy",
    Random_Metric == "random_F1" ~ "F1",
    Random_Metric == "random_Yoden" ~ "YodenJ",
    TRUE ~ NA_character_
  ), levels = c("Accuracy", "F1", "YodenJ"))) %>%
  filter(!is.na(Metric))

# Convert training_set to factor for consistency
long_Other$training_set <- as.factor(long_Other$training_set)
random_lines$training_set <- as.factor(random_lines$training_set)

# Convert training_set to numeric for reference lines
random_lines <- random_lines %>%
  mutate(training_set_numeric = as.numeric(training_set)) %>%
  mutate(model_type_numeric = as.numeric(model_type))

long_Other$behaviour <- factor(long_Other$behaviour, 
                               levels = c("walk", "eat", "lay", "Other", "weighted_avg"))
  
colours = c("tomato", "aquamarine3", "orchid3", "slateblue2", "goldenrod2")
Threshold_hypo2 <- ggplot(long_Other, aes(x = training_set, y = Value, colour = behaviour)) +
  geom_boxplot(linewidth = 1, fatten = 1, outlier.size = 2) +
  scale_color_manual(values = colours) +
  geom_segment(data = random_lines,
               aes(x = training_set_numeric - 0.3,
                   xend = training_set_numeric + 0.3,
                   y = Random_Value, 
                   yend = Random_Value,
                   group = Metric),
               color = "coral",
               linewidth = 1) +
  facet_wrap(~ Metric, scales = "free_y") +
  my_theme() +
  labs(x = "Training Set", 
       y = "Value")

print(Threshold_hypo2)

```
I looked more closely into the 'Binary' model, plotting all of the individual behaviours to see why this result was the case for the F1 score.

```{r, echo = FALSE}
Binary_df <- df_full %>% filter(closed_open == "open",
                          model_type == "Binary",
                          behaviour %in% c("eat", "lay", "walk", "Other", "weighted_avg"))

long_Binary <- melt(Binary_df, 
                id.vars = c("model_type", "training_set", "behaviour", "fold", "random_accuracy", "random_specificity", "random_F1"), 
                measure.vars = c("Accuracy", "F1", "YodenJ"), 
                variable.name = "Metric",
                value.name = "Value")

# Ensure Metric is a factor
long_Binary$Metric <- factor(long_Binary$Metric, levels = c("Accuracy", "F1", "YodenJ"))

# Create reference line data
random_lines <- Binary_df %>%
  select(training_set, model_type, behaviour, random_accuracy, random_F1, random_Yoden) %>%
  distinct() %>%  # Ensure one row per training_set, model_type
  pivot_longer(cols = c(random_accuracy, random_F1, random_Yoden), 
               names_to = "Random_Metric", 
               values_to = "Random_Value") %>%
  mutate(Metric = factor(case_when(
    Random_Metric == "random_accuracy" ~ "Accuracy",
    Random_Metric == "random_F1" ~ "F1",
    Random_Metric == "random_Yoden" ~ "YodenJ",
    TRUE ~ NA_character_
  ), levels = c("Accuracy", "F1", "YodenJ"))) %>%
  filter(!is.na(Metric))

# Convert training_set to factor for consistency
long_Binary$training_set <- as.factor(long_Binary$training_set)
random_lines$training_set <- as.factor(random_lines$training_set)

# Convert training_set to numeric for reference lines
random_lines <- random_lines %>%
  mutate(training_set_numeric = as.numeric(training_set)) %>%
  mutate(model_type_numeric = as.numeric(model_type))

long_Binary$behaviour <- factor(long_Binary$behaviour, 
                               levels = c("walk", "eat", "lay", "Other", "weighted_avg"))
  
colours = c("tomato", "aquamarine3", "orchid3", "slateblue2", "goldenrod2")
Binary_hypo2 <- ggplot(long_Binary, aes(x = training_set, y = Value, colour = behaviour)) +
  geom_boxplot(linewidth = 1, fatten = 1, outlier.size = 2) +
  scale_color_manual(values = colours) +
  geom_segment(data = random_lines,
               aes(x = training_set_numeric - 0.3,
                   xend = training_set_numeric + 0.3,
                   y = Random_Value, 
                   yend = Random_Value,
                   group = Metric),
               color = "coral",
               linewidth = 1) +
  facet_wrap(~ Metric, scales = "free_y") +
  my_theme() +
  labs(x = "Training Set", 
       y = "Value")

print(Binary_hypo2)

```

