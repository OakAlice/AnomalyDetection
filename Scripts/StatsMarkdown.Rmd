---
title: "Stats For Chapter 2"
output: html_document
date: "2025-02-27"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# packages
library(data.table)
library(tidyverse)
library(lme4)
library(emmeans)
library(lmerTest)
library(nlme)

# load in the data
path <- "C:/Users/PC/OneDrive - University of the Sunshine Coast/AnomalyDetection/Scripts/all_combined_metrics.csv"
# path <- "C:/Users/oaw001/OneDrive - University of the Sunshine Coast/AnomalyDetection/Output/Combined/all_combined_metrics.csv"
df <- read.csv(path)


target_behaviours <- c("eat", "lay", "walk", "Other")
target_behaviours2 <- c("eat", "lay", "walk")

# recode some of the variables
df <- df %>%
  mutate(model_type = recode(model_type,
                             "multi_Activity_NOthreshold" = "Control",
                             "binary" = "Binary",
                             "oneclass" = "One-class",
                             "multi_Activity_threshold" = "Threshold",
                             "multi_Other" = "Other"))



# define my own theme for all my plots
my_theme <- function() {
  theme(
# both axes
  axis.title = element_text(size = 14), 
  axis.text = element_text(size = 10),
  
  # x axis
  axis.text.x = element_text(angle = -20, vjust = 1, hjust = 0),
  
  # legend
  legend.title = element_text(size = 12, face = "bold"), 
  legend.text = element_text(size = 10),
  legend.position = "right",
  legend.box.background = element_blank(),
  legend.key.size = unit(1, "cm"),
  
  # background of the plot
  panel.background = element_blank(),
  panel.border = element_rect(color = "black", fill = NA, size = 2),
  panel.grid.major.y = element_line(color = "lightgrey", linetype = "dashed"),
  panel.grid.major.x = element_blank(),
  panel.grid.minor = element_blank(),
  plot.background = element_blank(),
  plot.margin = margin(1, 1, 1, 1, "cm"),
  
  # facet wrapping
  strip.background = element_blank(),
  strip.text = element_text(size = 12)
  )
}
  

```

## Accounting for chance
The main metric that I'm using, AUC already has a built-in mechanism for accounting for chance (0.5 is chance rate) but it also a little used metric in my field. To make things more accessible, I could use accuracy. Accuracy is heavily affected by class imbalance and I have to account for the rate of chance as determined by class prevalence. Although I think it is somewhat unfair to make it a truly random guesser (all classes in equal prevelance) because only a truly naive model would do that, and we rarely deal with truly naive guessers in reality... for simplicity in this paper, I will use this system. Furthermore, I don't think that the one-class and binary models (which are actually 4 independnt models and a default class) can reasonably be assessed in this manner... However, I can't resolve it, so may just have to take this solution.

In that case, I created a new column in my dataframe with the chance-adjusted accuracy for each behaviour within each model.

```{r adjusting for chance}

chance <- df %>% 
  group_by(model_type, training_set, closed_open, fold) %>%
  count() %>%
  mutate(chance_accuracy = 1/n) %>%
  ungroup() %>%
  select(-c("fold", "n"))

df <- merge(chance, df, by = c("model_type", "training_set", "closed_open"))

df <- df %>%
  mutate(Chance_Accuracy = Accuracy - chance_accuracy) %>%
  select(-chance_accuracy)

# rearrange it to be long
long_df <- melt(df, 
                id.vars = c("model_type", "training_set", "closed_open", "behaviour", "fold"), 
                measure.vars = c("AUC", "Chance_Accuracy"), 
                variable.name = "Metric",
                value.name = "Value") 

```


## Hypothesis 1

I hypothesised that the control model would decline in performance when tested on open rather than closed sets. To test this, I isolated my results from my control models and ran a nested anova to see whether there was a significant difference between conditions. I used only the weighted average of each fold because each model had a different number of behaviours.

I plotted this relationship.

```{r hypo1.plot1}
subset_df <- long_df %>% filter(behaviour == 'weighted_avg',
                           model_type == 'Control')
subset_df$training_set <- relevel(factor(subset_df$training_set), ref = "all")

ggplot(subset_df, aes(x = training_set, y = Value))+
  geom_boxplot()+
  facet_grid(Metric~closed_open) +
  my_theme()

```

Firstly I showed that when these models were tested on closed sets.

```{r hypo1.part1}
subset_df <- df %>% filter(closed_open == "closed",
                           behaviour == 'weighted_avg',
                           model_type == 'Control')

aov_model <- aov(AUC ~training_set, data = subset_df)
summary(aov_model)
TukeyHSD(aov_model)

```

I then tested whether this changed when tested on open sets.

```{r hypo1.part2}
subset_df <- df %>% filter(closed_open == "open",
                           behaviour == 'weighted_avg',
                           model_type == 'Control')
subset_df$training_set <- relevel(factor(subset_df$training_set), ref = "all")

aov_model <- aov(AUC ~training_set, data = subset_df)
summary(aov_model)

TukeyHSD(aov_model)
```

## Hypothesis 2

I hypothesised that the three multiclass models (control, threshold, and other) would all significantly degrade in performance between the all and reduced training conditions whereas the divide and conquer methods would not.

I couldn't decide whether to use just the weighted_avg AUC from each test or the AUC from each of the target behaviours

Firstly I plotted it.

```{r hypo2.plot1}

subset_df2 <- long_df %>% filter(closed_open == "open",
                           behaviour == "weighted_avg")
subset_df2$training_set <- relevel(factor(subset_df2$training_set), ref = "all")
subset_df2$model_type <- factor(subset_df2$model_type, levels = c("Control", "One-class", "Binary", "Threshold", "Other"))
subset_df2$model_type <- relevel(factor(subset_df2$model_type), ref = "Control")

ggplot(data = subset_df2, aes(x = training_set, y = Value)) +
  geom_boxplot() +
  facet_grid(Metric~model_type) +
  my_theme()

```

First, I checked whether there was a significant interaction between the two variables.

```{r hypo2.part1}
# aov_model <- aov(AUC ~ training_set * model_type, data = subset_df2)
# summary(aov_model)

subset_df2 <- subset_df2[subset_df2$Metric == "AUC", ]

lme_model <- lmer(Value ~ training_set * model_type + (1 | fold), data = subset_df2)
summary(lme_model)

```
There were no significant interactions found. Therefore, a reduced model was used instead and the fit between models was compared.

```{r hypo2.part2}
lme_model_reduced <- lmer(AUC ~ training_set + model_type + (1 | fold), data = subset_df2)
summary(lme_model_reduced)

anova(lme_model, lme_model_reduced)
```
There was no significant difference between the model with and without the interaction. Therefore, for simplicity's sake, the reduced model was used. Using this reduced model, pairwise comparisons were assessed.

```{r hypo2.part3}
pairwise <- emmeans(lme_model_reduced, pairwise ~ model_type | training_set, adjust = "tukey")
pairwise$contrasts
```




